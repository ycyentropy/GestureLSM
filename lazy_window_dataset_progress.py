import sys
import os
import time
import pickle
from typing import Dict, List, Optional, Union
import numpy as np
import torch
from torch.utils.data import Dataset
from tqdm import tqdm

from dataloaders.seamless_interaction import SeamlessInteractionDataset


class CachedLazySeamlessInteractionWindowDatasetWithProgress(Dataset):
    """
    å¸¦è¿›åº¦æ˜¾ç¤ºçš„ç¼“å­˜æ•°æ®é›†ç‰ˆæœ¬
    åœ¨æ•°æ®åŠ è½½è¿‡ç¨‹ä¸­æ˜¾ç¤ºè¯¦ç»†çš„è¿›åº¦ä¿¡æ¯
    """

    def __init__(
        self,
        data_path: Optional[str] = None,
        split: str = "train",
        sample_rate: int = 16000,
        pose_fps: int = 30,
        audio_fps: int = 16000,
        window_size: int = 64,
        window_stride: int = 20,
        multi_length_training: List[float] = [0.5, 0.75, 1.0, 1.25, 1.5],
        transform=None,
        load_video: bool = False,
        load_audio: bool = False,
        normalize: bool = True,
        max_samples: Optional[int] = None,
        cache_path: Optional[str] = None,
        show_progress: bool = True,
        progress_interval: int = 100  # æ¯éš”å¤šå°‘ä¸ªæ ·æœ¬æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
    ):
        """
        åˆå§‹åŒ–å¸¦è¿›åº¦æ˜¾ç¤ºçš„ç¼“å­˜æ•°æ®é›†

        Args:
            data_path: æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„
            split: æ•°æ®é›†åˆ†å‰² ('train', 'val', 'test')
            sample_rate: éŸ³é¢‘é‡‡æ ·ç‡
            pose_fps: å§¿æ€å¸§ç‡
            audio_fps: éŸ³é¢‘å¸§ç‡
            window_size: æ—¶é—´çª—å£å¤§å°(å¸§æ•°)
            window_stride: æ—¶é—´çª—å£æ­¥é•¿(å¸§æ•°)
            multi_length_training: å¤šé•¿åº¦è®­ç»ƒæ¯”ä¾‹åˆ—è¡¨
            transform: æ•°æ®å˜æ¢
            load_video: æ˜¯å¦åŠ è½½è§†é¢‘
            load_audio: æ˜¯å¦åŠ è½½éŸ³é¢‘
            normalize: æ˜¯å¦æ ‡å‡†åŒ–æ•°æ®
            max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼Œç”¨äºé™åˆ¶æ•°æ®é›†å¤§å°
            cache_path: ç¼“å­˜æ–‡ä»¶è·¯å¾„
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
            progress_interval: è¿›åº¦æ˜¾ç¤ºé—´éš”
        """
        # ä¿å­˜å‚æ•°
        self.data_path = data_path
        self.split = split
        self.sample_rate = sample_rate
        self.pose_fps = pose_fps
        self.audio_fps = audio_fps
        self.window_size = window_size
        self.window_stride = window_stride
        self.multi_length_training = multi_length_training
        self.transform = transform
        self.load_video = load_video
        self.load_audio = load_audio
        self.normalize = normalize
        self.max_samples = max_samples
        self.show_progress = show_progress
        self.progress_interval = progress_interval

        # å¦‚æœæ˜¯æµ‹è¯•é›†ï¼Œåªä½¿ç”¨1.0æ¯”ä¾‹
        if split == "test":
            self.multi_length_training = [1.0]

        # è¿›åº¦è·Ÿè¸ª
        self.access_count = 0
        self.start_time = None

        # ä»ç¼“å­˜æ–‡ä»¶åŠ è½½çª—å£å‚æ•°
        if cache_path and os.path.exists(cache_path):
            print(f"ğŸ“‚ ä»ç¼“å­˜æ–‡ä»¶åŠ è½½çª—å£å‚æ•°: {cache_path}")
            load_start = time.time()

            with open(cache_path, 'rb') as f:
                cache_data = pickle.load(f)

            # éªŒè¯ç¼“å­˜å‚æ•°æ˜¯å¦åŒ¹é…ï¼ˆå…è®¸çª—å£å‚æ•°ä¸åŒ¹é…ï¼Œåªç»™å‡ºè­¦å‘Šï¼‰
            if cache_data.get('window_size') != window_size:
                print(f"âš ï¸  è­¦å‘Š: ç¼“å­˜æ–‡ä»¶çª—å£å¤§å°({cache_data.get('window_size')})ä¸è¯·æ±‚çš„çª—å£å¤§å°({window_size})ä¸åŒ¹é…ï¼Œå°†ä½¿ç”¨ç¼“å­˜å‚æ•°")
                window_size = cache_data.get('window_size')
            if cache_data.get('window_stride') != window_stride:
                print(f"âš ï¸  è­¦å‘Š: ç¼“å­˜æ–‡ä»¶çª—å£æ­¥é•¿({cache_data.get('window_stride')})ä¸è¯·æ±‚çš„çª—å£æ­¥é•¿({window_stride})ä¸åŒ¹é…ï¼Œå°†ä½¿ç”¨ç¼“å­˜å‚æ•°")
                window_stride = cache_data.get('window_stride')
            if cache_data.get('split') != split:
                raise ValueError(f"âŒ ç¼“å­˜æ–‡ä»¶æ•°æ®é›†åˆ†å‰²({cache_data.get('split')})ä¸è¯·æ±‚çš„åˆ†å‰²({split})ä¸åŒ¹é…")

            # åŠ è½½ç¼“å­˜æ•°æ®
            self.window_counts = cache_data['window_counts']
            self.cumulative_windows = cache_data['cumulative_windows']
            self.window_params = cache_data['window_params']
            self.total_windows = cache_data['total_windows']
            self.base_dataset_indices = cache_data['base_dataset_indices']

            load_time = time.time() - load_start
            print(f"âœ… ç¼“å­˜åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f}ç§’")

            # å¦‚æœéœ€è¦é™åˆ¶æ ·æœ¬æ•°
            if max_samples is not None and max_samples < len(self.base_dataset_indices):
                print(f"ğŸ”§ é™åˆ¶åŸºç¡€æ•°æ®é›†å¤§å°: {len(self.base_dataset_indices)} -> {max_samples}")

                self.base_dataset_indices = self.base_dataset_indices[:max_samples]
                # é‡æ–°è®¡ç®—çª—å£ç›¸å…³æ•°æ®
                self.window_counts = self.window_counts[:max_samples]
                self.cumulative_windows = [0]
                for count in self.window_counts:
                    self.cumulative_windows.append(self.cumulative_windows[-1] + count)
                self.total_windows = self.cumulative_windows[-1]
                # é‡æ–°è®¡ç®—çª—å£å‚æ•°
                new_window_params = []
                for i in range(max_samples):
                    start_idx = sum(self.window_counts[:i])
                    end_idx = sum(self.window_counts[:i+1])
                    new_window_params.extend(self.window_params[start_idx:end_idx])
                self.window_params = new_window_params

                print(f"ğŸ“Š é™åˆ¶åæ€»çª—å£æ•°: {self.total_windows}")
        else:
            raise FileNotFoundError(f"âŒ ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_path}")

        # åˆ›å»ºåŸºç¡€æ•°æ®é›†
        print(f"ğŸ—ï¸  åˆ›å»ºåŸºç¡€æ•°æ®é›†...")
        dataset_start = time.time()

        self.base_dataset = SeamlessInteractionDataset(
            data_path=data_path,
            split=split,
            sample_rate=sample_rate,
            pose_fps=pose_fps,
            audio_fps=audio_fps,
            window_size=window_size / pose_fps,  # è½¬æ¢ä¸ºç§’æ•°ä¼ é€’ç»™åŸºç¡€æ•°æ®é›†
            window_stride=window_stride / pose_fps,  # è½¬æ¢ä¸ºç§’æ•°ä¼ é€’ç»™åŸºç¡€æ•°æ®é›†
            transform=transform,
            load_video=load_video,
            load_audio=load_audio,
            normalize=normalize
        )

        dataset_time = time.time() - dataset_start
        print(f"âœ… åŸºç¡€æ•°æ®é›†åˆ›å»ºå®Œæˆï¼Œè€—æ—¶: {dataset_time:.2f}ç§’")
        print(f"ğŸ“ˆ æ€»çª—å£æ•°: {self.total_windows:,}")
        print(f"ğŸ¯ æ•°æ®é›†åˆå§‹åŒ–å®Œæˆï¼")

        # åˆå§‹åŒ–è¿›åº¦æ¡
        if self.show_progress:
            self.progress_bar = None
            self.last_progress_update = 0

    def __len__(self) -> int:
        """è¿”å›çª—å£æ•°é‡"""
        return self.total_windows

    def _update_progress(self, idx: int):
        """æ›´æ–°è¿›åº¦æ˜¾ç¤º"""
        if not self.show_progress:
            return

        # åˆå§‹åŒ–è¿›åº¦æ¡
        if self.progress_bar is None:
            self.progress_bar = tqdm(
                total=self.total_windows,
                desc=f"ğŸ“Š {self.split} æ•°æ®åŠ è½½",
                unit="çª—å£",
                unit_scale=True,
                dynamic_ncols=True,
                bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]"
            )
            self.start_time = time.time()

        # æ›´æ–°è¿›åº¦æ¡
        current_progress = idx + 1
        self.progress_bar.update(current_progress - self.progress_bar.n)

        # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼ˆæ¯éš”ä¸€å®šé—´éš”ï¼‰
        if current_progress % self.progress_interval == 0 or current_progress == self.total_windows:
            elapsed = time.time() - self.start_time if self.start_time else 0
            if elapsed > 0:
                rate = current_progress / elapsed
                eta = (self.total_windows - current_progress) / rate if rate > 0 else 0
                self.progress_bar.set_postfix({
                    'é€Ÿåº¦': f'{rate:.1f} çª—å£/ç§’',
                    'å‰©ä½™': f'{eta/60:.1f}åˆ†é’Ÿ' if eta > 60 else f'{eta:.0f}ç§’'
                })

        # å®Œæˆæ—¶å…³é—­è¿›åº¦æ¡
        if current_progress == self.total_windows:
            self.progress_bar.close()
            total_time = time.time() - self.start_time if self.start_time else 0
            print(f"\nğŸ‰ æ•°æ®åŠ è½½å®Œæˆï¼")
            print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f}ç§’")
            print(f"ğŸ“Š å¹³å‡é€Ÿåº¦: {self.total_windows/total_time:.1f} çª—å£/ç§’")

    def __getitem__(self, idx: int) -> Dict[str, Union[torch.Tensor, np.ndarray, Dict]]:
        """è·å–å•ä¸ªçª—å£æ•°æ®ï¼ˆå¸¦è¿›åº¦æ˜¾ç¤ºï¼‰"""
        # æ›´æ–°è¿›åº¦
        self._update_progress(idx)

        # æ‰¾åˆ°çª—å£å¯¹åº”çš„åŸºç¡€æ ·æœ¬ç´¢å¼•
        sample_idx = self._find_sample_index(idx)

        # è®¡ç®—çª—å£åœ¨æ ·æœ¬ä¸­çš„ä½ç½®
        window_idx_in_sample = idx - self.cumulative_windows[sample_idx]

        # åŠ è½½åŸºç¡€æ ·æœ¬æ•°æ®
        sample = self.base_dataset[self.base_dataset_indices[sample_idx]]

        # è·å–åºåˆ—é•¿åº¦
        if 'pose' in sample:
            seq_len = len(sample['pose'])
        elif 'keypoints' in sample:
            seq_len = len(sample['keypoints'])
        elif 'emotion_scores' in sample:
            seq_len = len(sample['emotion_scores'])
        else:
            return sample  # å¦‚æœæ²¡æœ‰åºåˆ—æ•°æ®ï¼Œè¿”å›æ•´ä¸ªæ ·æœ¬

        # ç›´æ¥ä½¿ç”¨ç¼“å­˜çš„çª—å£å‚æ•°ï¼Œä¸éœ€è¦é‡æ–°è®¡ç®—
        cut_length, stride = self.window_params[idx]

        # å¯¹äºç¼“å­˜çš„çª—å£å‚æ•°ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—å®é™…çš„startå’Œendä½ç½®
        sample_start_window_idx = self.cumulative_windows[sample_idx]
        relative_window_idx = idx - sample_start_window_idx

        # è®¡ç®—èµ·å§‹ä½ç½®
        start = relative_window_idx * stride

        # ç¡®ä¿startä¸è¶…è¿‡åºåˆ—é•¿åº¦
        start = min(start, seq_len - cut_length if seq_len > cut_length else 0)
        start = max(0, start)

        # è®¡ç®—ç»“æŸä½ç½®
        end = min(start + cut_length, seq_len)

        # åˆ›å»ºçª—å£æ•°æ®
        window_data = {}

        # å¤åˆ¶éåºåˆ—æ•°æ®
        for key, value in sample.items():
            if key not in ['pose', 'keypoints', 'emotion_scores', 'expression', 'audio']:
                window_data[key] = value

        # æå–åºåˆ—æ•°æ®çš„çª—å£
        if 'pose' in sample:
            window_data['pose'] = sample['pose'][start:end]

        if 'keypoints' in sample:
            window_data['keypoints'] = sample['keypoints'][start:end]

        if 'emotion_scores' in sample:
            window_data['emotion_scores'] = sample['emotion_scores'][start:end]

        if 'expression' in sample:
            window_data['expression'] = sample['expression'][start:end]

        # å¤„ç†éŸ³é¢‘çª—å£ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'audio' in sample and self.load_audio:
            audio_start = int(start * self.audio_fps / self.pose_fps)
            audio_end = int(end * self.audio_fps / self.pose_fps)
            if audio_end < len(sample['audio']):
                window_data['audio'] = sample['audio'][audio_start:audio_end]
            else:
                window_data['audio'] = sample['audio'][audio_start:]

        # å¤„ç†translationæ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'translation' in sample:
            translation_data = sample['translation']
            if len(translation_data.shape) == 2:  # (T, 3)
                window_data['translation'] = translation_data[start:end]
            else:  # å‡è®¾æ˜¯(3,) æˆ–å…¶ä»–æ ¼å¼
                window_data['translation'] = translation_data

        # åº”ç”¨å˜æ¢
        if self.transform:
            window_data = self.transform(window_data)

        return window_data

    def _find_sample_index(self, window_idx: int) -> int:
        """äºŒåˆ†æŸ¥æ‰¾çª—å£å¯¹åº”çš„åŸºç¡€æ ·æœ¬ç´¢å¼•"""
        import bisect
        return bisect.bisect_right(self.cumulative_windows, window_idx) - 1


def create_progress_dataset(*args, **kwargs):
    """åˆ›å»ºå¸¦è¿›åº¦æ˜¾ç¤ºçš„æ•°æ®é›†çš„ä¾¿æ·å‡½æ•°"""
    return CachedLazySeamlessInteractionWindowDatasetWithProgress(*args, **kwargs)