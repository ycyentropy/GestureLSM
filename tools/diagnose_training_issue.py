#!/usr/bin/env python3
"""
è¯Šæ–­è®­ç»ƒè„šæœ¬å¡ä½çš„é—®é¢˜
"""

import os
import sys
import time
import pickle
import torch
import numpy as np
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from lazy_window_dataset_progress import create_progress_dataset

def test_dataset_loading():
    """æµ‹è¯•æ•°æ®é›†åŠ è½½"""
    print("ğŸ” å¼€å§‹è¯Šæ–­æ•°æ®é›†åŠ è½½é—®é¢˜...")
    
    # æµ‹è¯•ç¼“å­˜æ–‡ä»¶åŠ è½½
    cache_train = 'datasets/window_params/window_params_train_ws64_ws20_fixed.pkl'
    cache_val = 'datasets/window_params/window_params_val_ws64_ws20_fixed.pkl'
    
    print(f"ğŸ“‚ æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨...")
    print(f"  è®­ç»ƒç¼“å­˜: {os.path.exists(cache_train)}")
    print(f"  éªŒè¯ç¼“å­˜: {os.path.exists(cache_val)}")
    
    if os.path.exists(cache_train):
        print(f"ğŸ“Š åŠ è½½è®­ç»ƒç¼“å­˜æ–‡ä»¶...")
        start_time = time.time()
        try:
            with open(cache_train, 'rb') as f:
                cache_data = pickle.load(f)
            load_time = time.time() - start_time
            print(f"âœ… è®­ç»ƒç¼“å­˜åŠ è½½æˆåŠŸï¼Œè€—æ—¶: {load_time:.2f}ç§’")
            print(f"  çª—å£æ•°é‡: {cache_data.get('total_windows', 'N/A')}")
            print(f"  åŸºç¡€æ•°æ®é›†å¤§å°: {len(cache_data.get('base_dataset_indices', []))}")
        except Exception as e:
            print(f"âŒ è®­ç»ƒç¼“å­˜åŠ è½½å¤±è´¥: {e}")
            return
    
    # æµ‹è¯•æ•°æ®é›†åˆ›å»º
    print(f"ğŸ—ï¸  åˆ›å»ºæ•°æ®é›†...")
    dataset_start = time.time()
    
    try:
        # åªåˆ›å»ºä¸€ä¸ªå°çš„æ•°æ®é›†è¿›è¡Œæµ‹è¯•
        train_dataset = create_progress_dataset(
            data_path='datasets/seamless_interaction',
            split="train",
            window_size=64,
            window_stride=20,
            multi_length_training=[1.0],  # åªä½¿ç”¨å•ä¸€é•¿åº¦
            load_video=False,
            load_audio=False,
            max_samples=10,  # åªä½¿ç”¨10ä¸ªæ ·æœ¬
            cache_path=cache_train,
            show_progress=True,
            progress_interval=1
        )
        
        dataset_time = time.time() - dataset_start
        print(f"âœ… æ•°æ®é›†åˆ›å»ºæˆåŠŸï¼Œè€—æ—¶: {dataset_time:.2f}ç§’")
        print(f"ğŸ“Š æ•°æ®é›†å¤§å°: {len(train_dataset)}")
        
        # æµ‹è¯•æ•°æ®åŠ è½½
        print(f"ğŸ”„ æµ‹è¯•æ•°æ®åŠ è½½...")
        load_start = time.time()
        
        for i in range(min(5, len(train_dataset))):
            try:
                sample = train_dataset[i]
                if i == 0:
                    print(f"  æ ·æœ¬é”®: {list(sample.keys())}")
                    if 'pose' in sample:
                        print(f"  å§¿æ€å½¢çŠ¶: {sample['pose'].shape}")
            except Exception as e:
                print(f"âŒ åŠ è½½æ ·æœ¬ {i} å¤±è´¥: {e}")
                return
        
        load_time = time.time() - load_start
        print(f"âœ… æ•°æ®åŠ è½½æµ‹è¯•æˆåŠŸï¼Œè€—æ—¶: {load_time:.2f}ç§’")
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    print("ğŸ‰ æ•°æ®é›†è¯Šæ–­å®Œæˆï¼Œä¸€åˆ‡æ­£å¸¸ï¼")

def test_distributed_setup():
    """æµ‹è¯•åˆ†å¸ƒå¼è®¾ç½®"""
    print("\nğŸ” å¼€å§‹è¯Šæ–­åˆ†å¸ƒå¼è®¾ç½®é—®é¢˜...")
    
    try:
        print(f"ğŸ”§ æ£€æŸ¥CUDAå¯ç”¨æ€§...")
        print(f"  CUDAå¯ç”¨: {torch.cuda.is_available()}")
        print(f"  GPUæ•°é‡: {torch.cuda.device_count()}")
        
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
        
        print(f"ğŸ”§ æ£€æŸ¥NCCLå¯ç”¨æ€§...")
        try:
            if torch.distributed.is_nccl_available():
                print(f"  NCCLå¯ç”¨: æ˜¯")
            else:
                print(f"  NCCLå¯ç”¨: å¦")
        except Exception as e:
            print(f"  NCCLæ£€æŸ¥å¤±è´¥: {e}")
        
    except Exception as e:
        print(f"âŒ åˆ†å¸ƒå¼è®¾ç½®æ£€æŸ¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    print("ğŸ‰ åˆ†å¸ƒå¼è®¾ç½®è¯Šæ–­å®Œæˆï¼")

if __name__ == "__main__":
    test_dataset_loading()
    test_distributed_setup()