# RVQ-VAE 网络架构图

## 整体架构

```
┌─────────────────────────────────────────────────────────────────────────────────────────┐
│                                    RVQ-VAE 网络架构                                    │
└─────────────────────────────────────────────────────────────────────────────────────────┘

输入运动数据: (batch_size, sequence_length=128, motion_dim=330)
        │
        ▼
┌─────────────────────────────────────────────────────────────────────────────────────────┐
│                                         预处理                                          │
│  permute(0, 2, 1) → (batch_size, motion_dim, sequence_length)                        │
└─────────────────────────────────────────────────────────────────────────────────────────┘
        │
        ▼
┌─────────────────────────────────────────────────────────────────────────────────────────┐
│                                          编码器                                          │
│                                                                                         │
│  ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐    ┌─────────────┐ │
│  │   Conv1D(3,1)   │    │  Conv1D(stride) │    │   ResNet1D      │    │ Conv1D(3,1) │ │
│  │  (512, 128, 330)│ →  │ (512, 64, 512)  │ →  │ (512, 64, 512)│ →  │(512,64,128)│ │
│  └─────────────────┘    └─────────────────┘    └─────────────────┘    └─────────────┘ │
│                                                                                         │
│  参数: down_t=2, stride_t=2, width=512, depth=3, dilation_growth_rate=3              │
│  激活函数: ReLU, 归一化: None                                                           │
└─────────────────────────────────────────────────────────────────────────────────────────┘
        │
        ▼
┌─────────────────────────────────────────────────────────────────────────────────────────┐
│                                      残差向量量化器                                      │
│                                                                                         │
│  ┌─────────────────────────────────────────────────────────────────────────────────────┐ │
│  │                                   ResidualVQ (6层)                                  │ │
│  │                                                                                     │ │
│  │  层1: QuantizeEMAReset  层2: QuantizeEMAReset  层3: QuantizeEMAReset              │ │
│  │    (128→128)             (128→128)             (128→128)                           │ │
│  │         │                       │                       │                           │ │
│  │  层4: QuantizeEMAReset  层5: QuantizeEMAReset  层6: QuantizeEMAReset              │ │
│  │    (128→128)             (128→128)             (128→128)                           │ │
│  │                                                                                     │ │
│  │  每层包含:                                                                            │ │
│  │  - 码本大小: nb_code=1024                                                           │ │
│  │  - 码本维度: code_dim=128                                                           │ │
│  │  - EMA更新: mu=0.99                                                                 │ │
│  │  - Gumbel采样温度: 0.5                                                              │ │
│  └─────────────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                         │
│  输出:                                                                                  │
│  - 量化向量: (batch_size, 64, 128)                                                    │
│  - 量化索引: (batch_size, 64, 6)                                                      │
│  - 承诺损失: commit_loss                                                                │
│  - 困惑度: perplexity                                                                   │
└─────────────────────────────────────────────────────────────────────────────────────────┘
        │
        ▼
┌─────────────────────────────────────────────────────────────────────────────────────────┐
│                                          解码器                                          │
│                                                                                         │
│  ┌─────────────┐    ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐ │
│  │ Conv1D(3,1) │    │   ResNet1D      │    │  Upsample(×2)   │    │ Conv1D(3,1)     │ │
│  │(512,64,128)│ →  │ (512, 64, 512)│ →  │ (512, 128, 512)│ →  │(512,128,330)   │ │
│  └─────────────┘    └─────────────────┘    └─────────────────┘    └─────────────────┘ │
│                                                                                         │
│  参数: down_t=2, stride_t=2, width=512, depth=3, dilation_growth_rate=3              │
│  激活函数: ReLU, 归一化: None                                                           │
└─────────────────────────────────────────────────────────────────────────────────────────┘
        │
        ▼
┌─────────────────────────────────────────────────────────────────────────────────────────┐
│                                         后处理                                          │
│  permute(0, 2, 1) → (batch_size, sequence_length=128, motion_dim=330)                │
└─────────────────────────────────────────────────────────────────────────────────────────┘
        │
        ▼
输出: 重建运动数据: (batch_size, sequence_length=128, motion_dim=330)

## 网络组件详细说明

### 1. 编码器 (Encoder)
- **输入**: (batch_size, 330, 128) - 运动序列数据
- **结构**: 2层下采样 + 3层ResNet1D残差块
- **输出**: (batch_size, 128, 64) - 压缩的时序特征

```
Encoder结构:
Input(330,128) 
  → Conv1D(512,3,1,1) 
  → ReLU()
  → Block1: [Conv1D(512,4,2,1) → ResNet1D(512,3,3)]
  → Block2: [Conv1D(512,4,2,1) → ResNet1D(512,3,9)]
  → Conv1D(128,3,1,1)
  → Output(128,64)
```

### 2. 残差向量量化器 (ResidualVQ)
- **量化层数**: 6层 (num_quantizers=6)
- **码本大小**: 1024 (nb_code=1024)
- **码本维度**: 128 (code_dim=128)
- **EMA参数**: μ=0.99

```
ResidualVQ流程:
Input(128,64)
  → Layer1: 量化残差 → 索引1 + 量化1
  → Layer2: 量化残差 → 索引2 + 量化2
  → Layer3: 量化残差 → 索引3 + 量化3
  → Layer4: 量化残差 → 索引4 + 量化4
  → Layer5: 量化残差 → 索引5 + 量化5
  → Layer6: 量化残差 → 索引6 + 量化6
  → Sum(量化1-6) = 最终量化结果
```

### 3. 解码器 (Decoder)
- **输入**: (batch_size, 128, 64) - 量化特征
- **结构**: 2层上采样 + 3层ResNet1D残差块
- **输出**: (batch_size, 330, 128) - 重建运动数据

```
Decoder结构:
Input(128,64)
  → Conv1D(512,3,1,1)
  → ReLU()
  → Block1: [ResNet1D(512,3,9) → Upsample(×2) → Conv1D(512,3,1,1)]
  → Block2: [ResNet1D(512,3,3) → Upsample(×2) → Conv1D(512,3,1,1)]
  → Conv1D(512,3,1,1) → ReLU()
  → Conv1D(330,3,1,1)
  → Output(330,128)
```

### 4. ResNet1D块
- **残差连接**: 标准残差结构
- **膨胀率**: 指数增长 (1, 3, 9)
- **激活函数**: ReLU
- **归一化**: 可选 (LN/GN/BN/None)

```
ResConv1DBlock:
Input(n_in)
  → Norm1 → Activation1
  → Conv1D(n_state,3,1,dilation)
  → Norm2 → Activation2
  → Conv1D(n_in,1,1,0)
  → Dropout(0.2)
  → + Input (残差连接)
```

## 训练配置 (基于 train_rvq.sh)

```bash
python rvq_beatx_train.py \
  --batch-size 256 \              # 批次大小
  --lr 2e-4 \                     # 学习率
  --total-iter 300000 \           # 总迭代次数
  --nb-code 1024 \                # 码本大小
  --width 512 \                   # 网络宽度
  --code-dim 128 \                # 码本维度
  --down-t 2 \                    # 下采样次数
  --depth 3 \                      # ResNet深度
  --dilation-growth-rate 3 \      # 膨胀增长率
  --vq-act relu \                 # 激活函数
  --quantizer ema_reset \          # 量化器类型
  --loss-vel 0.5 \                 # 速度损失权重
  --recons-loss l1_smooth \       # 重建损失类型
  --body_part upper                # 身体部位
```

## 损失函数

1. **重建损失**: L1平滑损失 (Smooth L1 Loss)
2. **承诺损失**: 编码器输出与码本向量的MSE损失
3. **速度损失**: 运动速度的L1损失 (权重0.5)

## 输出

- **重建运动**: (batch_size, 128, 330)
- **量化索引**: (batch_size, 64, 6) - 用于离散表示
- **承诺损失**: 训练信号
- **困惑度**: 码本利用率指标