#!/usr/bin/env python3
"""
å¸¦è¿›åº¦æ˜¾ç¤ºçš„å¤šGPUè®­ç»ƒè„šæœ¬
åœ¨æ•°æ®åŠ è½½è¿‡ç¨‹ä¸­æ˜¾ç¤ºè¯¦ç»†çš„è¿›åº¦ä¿¡æ¯
"""

import os
import sys
import argparse
import logging
import time
import warnings
from datetime import datetime
from pathlib import Path

import numpy as np
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from lazy_window_dataset_progress import create_progress_dataset
from models.vq.model import RVQVAE
# from utils.config import load_config  # ä¸éœ€è¦é…ç½®æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
# from utils.media import Pipe  # ä¸éœ€è¦Pipeç±»

# å¿½ç•¥ä¸€äº›ä¸é‡è¦çš„è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)


def setup_logging(log_dir, local_rank):
    """è®¾ç½®æ—¥å¿—"""
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(log_dir, f"training_{local_rank}.log")

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler(sys.stdout) if local_rank == 0 else logging.NullHandler()
        ]
    )
    return logging.getLogger(f"Training_{local_rank}")


def create_model(args, input_dim, device):
    """åˆ›å»ºRVQVAEæ¨¡å‹"""
    model = RVQVAE(
        args,
        input_dim,
        args.nb_code,
        args.code_dim,
        args.code_dim,
        args.down_t,
        args.stride_t,
        args.width,
        args.depth,
        args.dilation_growth_rate,
        args.vq_act,
        args.vq_norm
    )

    model.to(device)

    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    return model, total_params, trainable_params


def create_optimizer_and_scheduler(model, args):
    """åˆ›å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=args.lr,
        weight_decay=args.weight_decay if hasattr(args, 'weight_decay') else 0.01
    )

    scheduler = torch.optim.lr_scheduler.MultiStepLR(
        optimizer,
        milestones=args.lr_scheduler,
        gamma=args.gamma
    )

    return optimizer, scheduler


def create_datasets(args):
    """åˆ›å»ºæ•°æ®é›†"""
    print(f"ğŸ—ï¸  åˆ›å»ºæ•°æ®é›†...")
    print(f"ğŸ“‚ æ•°æ®è·¯å¾„: {args.data_path}")
    print(f"ğŸ¯ è®­ç»ƒé›†åˆ†å‰²: {args.split}")
    print(f"ğŸ“ çª—å£å¤§å°: {args.window_size}, æ­¥é•¿: {args.window_stride}")
    print(f"ğŸ”„ å¤šé•¿åº¦è®­ç»ƒæ¯”ä¾‹: {args.multi_length_training}")
    print(f"ğŸ“Š æœ€å¤§æ ·æœ¬æ•°: {args.max_samples}")

    dataset_start = time.time()

    # è®­ç»ƒé›†
    train_dataset = create_progress_dataset(
        data_path=args.data_path,
        split="train",
        window_size=args.window_size,
        window_stride=args.window_stride,
        multi_length_training=args.multi_length_training,
        load_video=False,
        load_audio=False,
        max_samples=args.max_samples,
        cache_path=args.cache_train,
        show_progress=True,
        progress_interval=500  # æ¯500ä¸ªçª—å£æ˜¾ç¤ºä¸€æ¬¡è¯¦ç»†è¿›åº¦
    )

    # éªŒè¯é›†
    val_dataset = create_progress_dataset(
        data_path=args.data_path,
        split="val",
        window_size=args.window_size,
        window_stride=args.window_stride,
        multi_length_training=args.multi_length_training,
        load_video=False,
        load_audio=False,
        max_samples=args.max_samples,
        cache_path=args.cache_val,
        show_progress=True,
        progress_interval=200
    )

    dataset_time = time.time() - dataset_start
    print(f"âœ… æ•°æ®é›†åˆ›å»ºå®Œæˆï¼Œæ€»è€—æ—¶: {dataset_time:.2f}ç§’")
    print(f"ğŸ“Š è®­ç»ƒé›†çª—å£æ•°: {len(train_dataset):,}")
    print(f"ğŸ“Š éªŒè¯é›†çª—å£æ•°: {len(val_dataset):,}")

    return train_dataset, val_dataset


def collate_fn(batch):
    """æ‰¹æ¬¡æ•´ç†å‡½æ•°"""
    max_len = max(item['pose'].shape[0] for item in batch)
    batch_size = len(batch)
    pose_dim = batch[0]['pose'].shape[1]

    poses = torch.zeros(batch_size, max_len, pose_dim)
    masks = torch.zeros(batch_size, max_len, dtype=torch.bool)

    for i, item in enumerate(batch):
        pose = item['pose']
        length = pose.shape[0]
        poses[i, :length] = pose
        masks[i, :length] = True

    return {'pose': poses, 'mask': masks}


def train_epoch(model, dataloader, optimizer, device, epoch, logger, writer, args):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()

    epoch_loss = 0.0
    epoch_motion_loss = 0.0
    epoch_commit_loss = 0.0
    epoch_perplexity = 0.0
    num_batches = 0

    # åˆ›å»ºè¿›åº¦æ¡
    if dist.get_rank() == 0:
        pbar = tqdm(
            dataloader,
            desc=f"Epoch {epoch}",
            leave=False,
            dynamic_ncols=True
        )
    else:
        pbar = dataloader

    for batch_idx, batch in enumerate(pbar):
        # æ•°æ®ç§»åˆ°è®¾å¤‡
        gt_motion = batch['pose'].to(device, non_blocking=True)
        batch_mask = batch['mask'].to(device, non_blocking=True)

        # æ•°æ®æ ‡å‡†åŒ–
        mean_pose = args.mean_pose.to(device)
        std_pose = args.std_pose.to(device)
        gt_motion = (gt_motion - mean_pose) / std_pose

        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()

        output = model(gt_motion, mask=batch_mask)
        pred_motion = output["x_rec"]
        loss_commit = output["commit_loss"]
        perplexity = output["perplexity"]

        # è®¡ç®—æŸå¤±
        if args.recons_loss == "l1_smooth":
            loss_motion = torch.nn.functional.l1_loss(pred_motion, gt_motion, reduction='mean')
        else:
            loss_motion = torch.nn.functional.mse_loss(pred_motion, gt_motion, reduction='mean')

        loss = loss_motion + args.commit * loss_commit

        # åå‘ä¼ æ’­
        loss.backward()

        # æ¢¯åº¦è£å‰ª
        if args.max_grad_norm > 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

        optimizer.step()

        # ç»Ÿè®¡æŸå¤±
        epoch_loss += loss.item()
        epoch_motion_loss += loss_motion.item()
        epoch_commit_loss += loss_commit.item()
        epoch_perplexity += perplexity.item()
        num_batches += 1

        # æ›´æ–°è¿›åº¦æ¡
        if dist.get_rank() == 0:
            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Motion': f'{loss_motion.item():.4f}',
                'Commit': f'{loss_commit.item():.4f}',
                'PPL': f'{perplexity.item():.1f}'
            })

            # è®°å½•åˆ°tensorboard
            global_step = epoch * len(dataloader) + batch_idx
            writer.add_scalar('Train/TotalLoss', loss.item(), global_step)
            writer.add_scalar('Train/MotionLoss', loss_motion.item(), global_step)
            writer.add_scalar('Train/CommitLoss', loss_commit.item(), global_step)
            writer.add_scalar('Train/Perplexity', perplexity.item(), global_step)
            writer.add_scalar('Train/LR', optimizer.param_groups[0]['lr'], global_step)

    # å¹³å‡æŸå¤±
    avg_loss = epoch_loss / num_batches
    avg_motion_loss = epoch_motion_loss / num_batches
    avg_commit_loss = epoch_commit_loss / num_batches
    avg_perplexity = epoch_perplexity / num_batches

    return avg_loss, avg_motion_loss, avg_commit_loss, avg_perplexity


def validate(model, dataloader, device, logger, args):
    """éªŒè¯æ¨¡å‹"""
    model.eval()

    val_loss = 0.0
    val_motion_loss = 0.0
    val_commit_loss = 0.0
    val_perplexity = 0.0
    num_batches = 0

    with torch.no_grad():
        # åˆ›å»ºè¿›åº¦æ¡
        if dist.get_rank() == 0:
            pbar = tqdm(
                dataloader,
                desc="Validation",
                leave=False,
                dynamic_ncols=True
            )
        else:
            pbar = dataloader

        for batch in pbar:
            # æ•°æ®ç§»åˆ°è®¾å¤‡
            gt_motion = batch['pose'].to(device, non_blocking=True)
            batch_mask = batch['mask'].to(device, non_blocking=True)

            # æ•°æ®æ ‡å‡†åŒ–
            mean_pose = args.mean_pose.to(device)
            std_pose = args.std_pose.to(device)
            gt_motion = (gt_motion - mean_pose) / std_pose

            # å‰å‘ä¼ æ’­
            output = model(gt_motion, mask=batch_mask)
            pred_motion = output["x_rec"]
            loss_commit = output["commit_loss"]
            perplexity = output["perplexity"]

            # è®¡ç®—æŸå¤±
            if args.recons_loss == "l1_smooth":
                loss_motion = torch.nn.functional.l1_loss(pred_motion, gt_motion, reduction='mean')
            else:
                loss_motion = torch.nn.functional.mse_loss(pred_motion, gt_motion, reduction='mean')

            loss = loss_motion + args.commit * loss_commit

            # ç»Ÿè®¡æŸå¤±
            val_loss += loss.item()
            val_motion_loss += loss_motion.item()
            val_commit_loss += loss_commit.item()
            val_perplexity += perplexity.item()
            num_batches += 1

            # æ›´æ–°è¿›åº¦æ¡
            if dist.get_rank() == 0:
                pbar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Motion': f'{loss_motion.item():.4f}',
                    'PPL': f'{perplexity.item():.1f}'
                })

    # å¹³å‡æŸå¤±
    avg_loss = val_loss / num_batches
    avg_motion_loss = val_motion_loss / num_batches
    avg_commit_loss = val_commit_loss / num_batches
    avg_perplexity = val_perplexity / num_batches

    return avg_loss, avg_motion_loss, avg_commit_loss, avg_perplexity


def train_worker(local_rank, args):
    """è®­ç»ƒå·¥ä½œè¿›ç¨‹"""
    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    dist.init_process_group(backend='nccl')

    # è®¾ç½®è®¾å¤‡
    device = torch.device(f"cuda:{local_rank}")
    torch.cuda.set_device(local_rank)

    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(args.seed + local_rank)
    np.random.seed(args.seed + local_rank)

    # åˆ›å»ºæ—¥å¿—è®°å½•å™¨
    logger = setup_logging(args.out_dir, local_rank)

    if dist.get_rank() == 0:
        logger.info(f"å¼€å§‹è®­ç»ƒï¼Œå‚æ•°é…ç½®: {vars(args)}")
        logger.info(f"ä½¿ç”¨ {dist.get_world_size()} ä¸ªGPUï¼Œæ¯ä¸ªGPUçš„batch_sizeä¸º {args.batch_size}")
        logger.info(f"æ€»æœ‰æ•ˆbatch_sizeä¸º {args.batch_size * dist.get_world_size()}")

    # åŠ è½½å‡å€¼å’Œæ ‡å‡†å·®
    args.mean_pose = torch.from_numpy(np.load('mean_std/seamless_smplh_mean.npy')[:args.pose_dim]).float()
    args.std_pose = torch.from_numpy(np.load('mean_std/seamless_smplh_std.npy')[:args.pose_dim]).float()
    args.mean_pose = args.mean_pose.unsqueeze(0).unsqueeze(0)
    args.std_pose = args.std_pose.unsqueeze(0).unsqueeze(0)

    # åˆ›å»ºæ•°æ®é›†
    train_dataset, val_dataset = create_datasets(args)

    # åˆ›å»ºæ•°æ®é‡‡æ ·å™¨å’ŒåŠ è½½å™¨
    train_sampler = DistributedSampler(
        train_dataset,
        num_replicas=dist.get_world_size(),
        rank=dist.get_rank(),
        shuffle=True,
        drop_last=True
    )

    val_sampler = DistributedSampler(
        val_dataset,
        num_replicas=dist.get_world_size(),
        rank=dist.get_rank(),
        shuffle=False,
        drop_last=False
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        sampler=train_sampler,
        num_workers=0,
        pin_memory=True,
        drop_last=True,
        collate_fn=collate_fn
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        sampler=val_sampler,
        num_workers=0,
        pin_memory=True,
        drop_last=False,
        collate_fn=collate_fn
    )

    # åˆ›å»ºæ¨¡å‹
    if dist.get_rank() == 0:
        logger.info("åˆ›å»ºæ¨¡å‹...")

    model, total_params, trainable_params = create_model(args, args.pose_dim, device)

    # åŒ…è£…ä¸ºåˆ†å¸ƒå¼æ¨¡å‹
    model = DDP(model, device_ids=[local_rank], find_unused_parameters=True)

    # åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer, scheduler = create_optimizer_and_scheduler(model, args)

    # åˆ›å»ºtensorboardå†™å…¥å™¨
    if dist.get_rank() == 0:
        writer = SummaryWriter(os.path.join(args.out_dir, 'tensorboard'))
        logger.info(f"æ¨¡å‹å‚æ•°é‡: {total_params/1e6:.1f}M (å¯è®­ç»ƒ: {trainable_params/1e6:.1f}M)")
        logger.info(f"è®­ç»ƒé›†æ‰¹æ¬¡æ•°: {len(train_loader)}, éªŒè¯é›†æ‰¹æ¬¡æ•°: {len(val_loader)}")
        logger.info("å³å°†è¿›å…¥è®­ç»ƒå¾ªç¯...")

    # è®­ç»ƒå¾ªç¯
    best_val_loss = float('inf')
    start_epoch = 0

    if dist.get_rank() == 0:
        logger.info("å¼€å§‹è®­ç»ƒï¼Œæ€»å…± %d æ¬¡è¿­ä»£", args.total_iter)

    # è®­ç»ƒè¿›åº¦æ¡
    global_step = 0
    epoch_pbar = tqdm(range(start_epoch, args.total_iter),
                     desc="è®­ç»ƒè¿›åº¦",
                     disable=dist.get_rank() != 0)

    for epoch in epoch_pbar:
        # è®¾ç½®é‡‡æ ·å™¨çš„epoch
        train_sampler.set_epoch(epoch)

        # è®­ç»ƒ
        train_loss, train_motion_loss, train_commit_loss, train_perplexity = train_epoch(
            model, train_loader, optimizer, device, epoch, logger, writer, args
        )

        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()

        # éªŒè¯
        if epoch % args.eval_iter == 0 or epoch == args.total_iter - 1:
            val_loss, val_motion_loss, val_commit_loss, val_perplexity = validate(
                model, val_loader, device, logger, args
            )

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                if dist.get_rank() == 0:
                    torch.save({
                        'epoch': epoch,
                        'model_state_dict': model.module.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'scheduler_state_dict': scheduler.state_dict(),
                        'best_val_loss': best_val_loss,
                        'args': args
                    }, os.path.join(args.out_dir, 'best_model.pth'))
                    logger.info(f"ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯æŸå¤±: {best_val_loss:.4f}")

            # è®°å½•éªŒè¯æŸå¤±
            if dist.get_rank() == 0:
                writer.add_scalar('Val/TotalLoss', val_loss, global_step)
                writer.add_scalar('Val/MotionLoss', val_motion_loss, global_step)
                writer.add_scalar('Val/CommitLoss', val_commit_loss, global_step)
                writer.add_scalar('Val/Perplexity', val_perplexity, global_step)

                logger.info(f"Epoch {epoch}: Train={train_loss:.4f}, Val={val_loss:.4f}, Best={best_val_loss:.4f}")

        # ä¿å­˜å®šæœŸæ£€æŸ¥ç‚¹
        if epoch % 1000 == 0 and dist.get_rank() == 0:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.module.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'args': args
            }, os.path.join(args.out_dir, f'checkpoint_epoch_{epoch}.pth'))

        global_step += len(train_loader)

        # æ›´æ–°è¿›åº¦æ¡
        if dist.get_rank() == 0:
            epoch_pbar.set_postfix({
                'Train': f'{train_loss:.4f}',
                'Val': f'{val_loss if epoch % args.eval_iter == 0 else "N/A"}',
                'Best': f'{best_val_loss:.4f}'
            })

    # è®­ç»ƒå®Œæˆ
    if dist.get_rank() == 0:
        writer.close()
        logger.info("è®­ç»ƒå®Œæˆ!")

    # æ¸…ç†
    dist.destroy_process_group()


def main():
    parser = argparse.ArgumentParser()

    # åˆ†å¸ƒå¼è®­ç»ƒå‚æ•°
    parser.add_argument('--local_rank', type=int, default=0)

    # æ•°æ®å‚æ•°
    parser.add_argument('--data_path', type=str, default='/home/embodied/yangchenyu/GestureLSM/datasets/seamless_interaction')
    parser.add_argument('--split', type=str, default='train')
    parser.add_argument('--window_size', type=int, default=64)
    parser.add_argument('--window_stride', type=int, default=20)
    parser.add_argument('--multi_length_training', type=float, nargs='+', default=[0.5, 0.75, 1.0, 1.25, 1.5])
    parser.add_argument('--max_samples', type=int, default=None)
    parser.add_argument('--cache_train', type=str, default=None)
    parser.add_argument('--cache_val', type=str, default=None)
    parser.add_argument('--use_cache', action='store_true')

    # æ¨¡å‹å‚æ•°
    parser.add_argument('--code_dim', type=int, default=128)
    parser.add_argument('--nb_code', type=int, default=1024)
    parser.add_argument('--num_quantizers', type=int, default=8)
    parser.add_argument('--down_t', type=int, default=2)
    parser.add_argument('--stride_t', type=int, default=2)
    parser.add_argument('--width', type=int, default=512)
    parser.add_argument('--depth', type=int, default=3)
    parser.add_argument('--dilation_growth_rate', type=int, default=3)
    parser.add_argument('--vq_act', type=str, default='relu')
    parser.add_argument('--vq_norm', type=str, default=None)
    parser.add_argument('--commit', type=float, default=0.02)
    parser.add_argument('--mu', type=float, default=0.99)
    parser.add_argument('--quantize_dropout_prob', type=float, default=0.5)
    parser.add_argument('--recons_loss', type=str, default='l1_smooth')
    parser.add_argument('--loss_vel', type=float, default=0.0)
    parser.add_argument('--shared_codebook', action='store_true')

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--batch_size', type=int, default=64)
    parser.add_argument('--lr', type=float, default=2e-4)
    parser.add_argument('--weight_decay', type=float, default=0.01)
    parser.add_argument('--lr_scheduler', type=int, nargs='+', default=[5000, 8000])
    parser.add_argument('--gamma', type=float, default=0.1)
    parser.add_argument('--total_iter', type=int, default=10000)
    parser.add_argument('--eval_iter', type=int, default=1000)
    parser.add_argument('--max_grad_norm', type=float, default=1.0)
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--pose_dim', type=int, default=156)

    # è¾“å‡ºå‚æ•°
    parser.add_argument('--out_dir', type=str, default='experiments/rvq_seamless_progress')
    parser.add_argument('--exp_name', type=str, default='rvq_seamless_progress')
    parser.add_argument('--print_iter', type=int, default=100)

    # æ·»åŠ å¯¹torch.distributed.launchä¼ é€’çš„--local-rankå‚æ•°çš„æ”¯æŒ
    # æ³¨æ„ï¼štorch.distributed.launchä½¿ç”¨--local-rankï¼ˆå¸¦è¿å­—ç¬¦ï¼‰ï¼Œè€Œä¸æ˜¯--local_rankï¼ˆå¸¦ä¸‹åˆ’çº¿ï¼‰
    known_args, unknown_args = parser.parse_known_args()

    # å¤„ç†æœªçŸ¥å‚æ•°ï¼Œç‰¹åˆ«æ˜¯--local-rank
    local_rank = 0
    for i, arg in enumerate(unknown_args):
        if arg == '--local-rank' and i + 1 < len(unknown_args):
            local_rank = int(unknown_args[i + 1])
            break
        elif arg.startswith('--local-rank='):
            local_rank = int(arg.split('=')[1])
            break

    # è®¾ç½®local_rank
    known_args.local_rank = local_rank
    args = known_args

    # è‡ªåŠ¨å¯ç”¨ç¼“å­˜æ¨¡å¼
    if args.cache_train is not None or args.cache_val is not None:
        args.use_cache = True
        print(f"ğŸ”§ æ£€æµ‹åˆ°ç¼“å­˜è·¯å¾„ï¼Œè‡ªåŠ¨å¯ç”¨ç¼“å­˜æ¨¡å¼")
        print(f"ğŸ“‚ ç¼“å­˜è·¯å¾„ - è®­ç»ƒ: {args.cache_train}, éªŒè¯: {args.cache_val}")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.out_dir, exist_ok=True)

    # ä¿å­˜é…ç½®
    import json
    with open(os.path.join(args.out_dir, 'config.json'), 'w') as f:
        json.dump(vars(args), f, indent=2)

    # å¯åŠ¨è®­ç»ƒ
    print("ğŸš€ å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ...")
    mp.spawn(train_worker, args=(args,), nprocs=torch.cuda.device_count(), join=True)


if __name__ == '__main__':
    main()